{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtqSHg8_3i05"
   },
   "source": [
    "# Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "alc-1Vul3oaz"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "from datetime import datetime\n",
    "from utils.csv_headers import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "UNZIPPED_DIR = os.path.join(CURRENT_PATH,'../content/downloads/unzipped')\n",
    "OUTPUT_FILE = os.path.join(CURRENT_PATH,'../content/output_sql.tsv')\n",
    "ENHANCED_FILE = os.path.join(CURRENT_PATH,'../content/enhanced-output_sql.tsv')\n",
    "TABLE_DIR = os.path.join(CURRENT_PATH,'../content/tables/')\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "NEWLINE = ''\n",
    "DELIMITER = '\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwMeFO8IA9EC"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkRq4gKiWxtx"
   },
   "source": [
    "## Unzip CSV project files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XFGnghtDLM4"
   },
   "source": [
    "## CSV files merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(string):\n",
    "    try:\n",
    "        int(string)\n",
    "        return True\n",
    "    except ValueError as a:\n",
    "        return False\n",
    "    \n",
    "def is_corrupted(dict_row):\n",
    "    empty_column = None in dict_row\n",
    "    is_application_id_not_integer = not is_integer(dict_row[APPLICATION_ID])   \n",
    "    return empty_column or is_application_id_not_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aPtvpzIC0oh",
    "outputId": "3ee8057d-dfaf-4495-a02f-808bdb75b161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RePORTER_PRJ_C_FY1985.csv\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, 'w', encoding=ENCODING, newline=NEWLINE) as output_file:\n",
    "    writer = csv.DictWriter(output_file, fieldnames=ORDERED_HEADERS, delimiter=DELIMITER)\n",
    "    writer.writeheader()\n",
    "\n",
    "    csv_files = sorted(os.listdir(UNZIPPED_DIR))\n",
    "\n",
    "    for csv_file_name in csv_files:\n",
    "        print(csv_file_name)\n",
    "\n",
    "        with open(UNZIPPED_DIR + '/' + csv_file_name, 'r', encoding='ISO-8859-1', newline=NEWLINE) as csv_file_descriptor:\n",
    "            reader = csv.DictReader(csv_file_descriptor, delimiter=',', quotechar='\"')\n",
    "            \n",
    "            for index, dict_row in enumerate(reader):\n",
    "                if is_corrupted(dict_row):\n",
    "                    print(csv_file_name, index, dict_row[APPLICATION_ID])\n",
    "                else:\n",
    "                    upper_dict = dict()\n",
    "                    for key, value in dict_row.items():\n",
    "                        try:\n",
    "                            # letter case uniformization\n",
    "                            upper_dict[key.upper()] = value.replace(\"\\n\", \"\").replace(\"\\t\", \" \")\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(dict_row)\n",
    "                            print(csv_file_name)\n",
    "                            #raise e\n",
    "                    writer.writerow(upper_dict)\n",
    "                \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SeT387SDQ9L"
   },
   "source": [
    "## Contact PIs first name extraction & gender assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mm4-ol6lWmtG"
   },
   "outputs": [],
   "source": [
    "def string_to_list(string):\n",
    "    pi_list = []\n",
    "    for string_part in string.rstrip('; ').split(';'):\n",
    "        string_part = string_part.strip(', ').replace('\"', '')\n",
    "        if string_part != '':\n",
    "            pi_list.append(string_part)\n",
    "    return pi_list\n",
    "\n",
    "\n",
    "def filter_contact_pi(pi_ids_or_names_list):\n",
    "    filtered_list = []\n",
    "    for item in pi_ids_or_names_list: \n",
    "         if '(contact)' in item:\n",
    "            filtered_list.append(item)\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def extract_contact_pi(pi_ids_or_names_list):\n",
    "    if len(pi_ids_or_names_list) == 0:\n",
    "        return None, \"no_value\"\n",
    "    elif len(pi_ids_or_names_list) == 1:\n",
    "        return pi_ids_or_names_list[0], \"single_value\"\n",
    "    else:              \n",
    "        filtered_pi_ids_or_names = filter_contact_pi(pi_ids_or_names_list)\n",
    "        if len(filtered_pi_ids_or_names) == 0:\n",
    "            return pi_ids_or_names_list[0], \"multiple_values_but_no_explicit_contact_first_chosen\"\n",
    "        elif len(filtered_pi_ids_or_names) == 1:\n",
    "            return filtered_pi_ids_or_names[0], \"multiple_values_and_single_explicit_contact\"\n",
    "        else:\n",
    "            return filtered_pi_ids_or_names[0], \"multiple_values_and_multiple_explicit_contacts_first_chosen\"\n",
    "\n",
    "\n",
    "def normalize_first_name(first_name):\n",
    "    first_name = re.sub('[\\.\\-\\;\\,]', '', first_name)\n",
    "    first_name = re.sub(' +', ' ', first_name)\n",
    "    first_name_parts = first_name.strip(' ').split(' ')\n",
    "    l = []\n",
    "    for part in first_name_parts:\n",
    "        if len(part) > 1:\n",
    "            l.append(part)\n",
    "    return ' '.join(l).upper()\n",
    "\n",
    "\n",
    "def extract_pi_first_name(full_name):\n",
    "    if full_name is None:\n",
    "        return None\n",
    "    else:\n",
    "        full_name_list = full_name.split(',')\n",
    "        if len(full_name_list) <= 1:\n",
    "            return None\n",
    "        else:\n",
    "            return normalize_first_name(full_name_list[1].replace('(contact)', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TxWmUc8oXPrL"
   },
   "outputs": [],
   "source": [
    "# upload and clean gender_US_names.csv list\n",
    "GENDER_FILE = os.path.join(CURRENT_PATH,'../content/gender_US_names.csv')\n",
    "\n",
    "with open(GENDER_FILE, 'r', encoding='UTF-8-sig', newline=NEWLINE) as csv_file_descriptor:\n",
    "    gender_dataset = csv.DictReader(csv_file_descriptor, delimiter=',', quotechar='\"')\n",
    "\n",
    "    gender_dict = {}\n",
    "\n",
    "    for item in gender_dataset:\n",
    "        normalized_first_name = normalize_first_name(item['Name'])\n",
    "        gender_dict[normalized_first_name] = item['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uJ2PoiZ0Dckb"
   },
   "outputs": [],
   "source": [
    "# enhance output.csv file with contact PIs extraction and gender assignment\n",
    "with open(ENHANCED_FILE, 'w', encoding=ENCODING, newline=NEWLINE) as output_file:\n",
    "    writer = csv.DictWriter(output_file, fieldnames=[\n",
    "        APPLICATION_ID,\n",
    "        ACTIVITY,\n",
    "        ADMINISTERING_IC,\n",
    "        APPLICATION_TYPE,\n",
    "        ARRA_FUNDED,\n",
    "        AWARD_NOTICE_DATE,\n",
    "        BUDGET_START,\n",
    "        BUDGET_END,\n",
    "        CFDA_CODE,\n",
    "        CORE_PROJECT_NUM,\n",
    "        ED_INST_TYPE,\n",
    "        FOA_NUMBER,\n",
    "        FULL_PROJECT_NUM,\n",
    "        FUNDING_ICS,\n",
    "        FUNDING_MECHANISM,\n",
    "        FY,\n",
    "        IC_NAME,\n",
    "        NIH_SPENDING_CATS,\n",
    "        ORG_CITY,\n",
    "        ORG_COUNTRY,\n",
    "        ORG_DEPT,\n",
    "        ORG_DISTRICT,\n",
    "        ORG_DUNS,\n",
    "        ORG_FIPS,\n",
    "        ORG_IPF_CODE,\n",
    "        ORG_NAME,\n",
    "        ORG_STATE,\n",
    "        ORG_ZIPCODE,\n",
    "        PHR,\n",
    "        PI_IDS,\n",
    "        PI_NAMES,\n",
    "\n",
    "        NB_PI_IDS,\n",
    "        NB_PI_NAMES,\n",
    "        CONTACT_PI_ID, \n",
    "        CONTACT_PI_NAME, \n",
    "        CONTACT_PI_ID_STATUS, \n",
    "        CONTACT_PI_NAME_STATUS,\n",
    "        CONTACT_PI_FIRST_NAME,\n",
    "        CONTACT_PI_GENDER,\n",
    "\n",
    "        PROGRAM_OFFICER_NAME,\n",
    "        PROJECT_START,\n",
    "        PROJECT_END,\n",
    "        PROJECT_TERMS,\n",
    "        PROJECT_TITLE,\n",
    "        SERIAL_NUMBER,\n",
    "        STUDY_SECTION,\n",
    "        STUDY_SECTION_NAME,\n",
    "        SUBPROJECT_ID,\n",
    "        SUFFIX,\n",
    "        SUPPORT_YEAR,\n",
    "        DIRECT_COST_AMT,\n",
    "        INDIRECT_COST_AMT,\n",
    "        TOTAL_COST,\n",
    "        TOTAL_COST_SUB_PROJECT\n",
    "    ], delimiter=DELIMITER)\n",
    "    writer.writeheader()\n",
    "\n",
    "\n",
    "    with open(OUTPUT_FILE, 'r', encoding=ENCODING, newline=NEWLINE) as input_file:\n",
    "        reader = csv.DictReader(input_file, delimiter=DELIMITER)\n",
    "\n",
    "        for index, input_row in enumerate(reader):\n",
    "\n",
    "            pi_ids_list = string_to_list(input_row[PI_IDS])\n",
    "            pi_names_list = string_to_list(input_row[PI_NAMES])\n",
    "\n",
    "            try:\n",
    "                contact_pi_id, pi_id_status = extract_contact_pi(pi_ids_list)\n",
    "                contact_pi_name, pi_name_status = extract_contact_pi(pi_names_list)\n",
    "            except Exception as e:\n",
    "                print('APPLICATION_ID', input_row[APPLICATION_ID])\n",
    "                print('PI_IDS', input_row[PI_IDS])\n",
    "                print('PI_NAMES', input_row[PI_NAMES])\n",
    "                print('+++')\n",
    "                print(pi_ids_list)\n",
    "                print(pi_names_list)\n",
    "                print(e)\n",
    "                print('----------------')\n",
    "\n",
    "            contact_pi_first_name = extract_pi_first_name(contact_pi_name)\n",
    "\n",
    "            dict_row = {\n",
    "                APPLICATION_ID: input_row[APPLICATION_ID],\n",
    "                ACTIVITY: input_row[ACTIVITY],\n",
    "                ADMINISTERING_IC: input_row[ADMINISTERING_IC],\n",
    "                APPLICATION_TYPE: input_row[APPLICATION_TYPE],\n",
    "                ARRA_FUNDED: input_row[ARRA_FUNDED],\n",
    "                AWARD_NOTICE_DATE: input_row[AWARD_NOTICE_DATE],\n",
    "                BUDGET_START: input_row[BUDGET_START],\n",
    "                BUDGET_END: input_row[BUDGET_END],\n",
    "                CFDA_CODE: input_row[CFDA_CODE],\n",
    "                CORE_PROJECT_NUM: input_row[CORE_PROJECT_NUM],\n",
    "                ED_INST_TYPE: input_row[ED_INST_TYPE],\n",
    "                FOA_NUMBER: input_row[FOA_NUMBER],\n",
    "                FULL_PROJECT_NUM: input_row[FULL_PROJECT_NUM],\n",
    "                FUNDING_ICS: input_row[FUNDING_ICS],\n",
    "                FUNDING_MECHANISM: input_row[FUNDING_MECHANISM],\n",
    "                FY: input_row[FY],\n",
    "                IC_NAME: input_row[IC_NAME],\n",
    "                NIH_SPENDING_CATS: input_row[NIH_SPENDING_CATS],\n",
    "                ORG_CITY: input_row[ORG_CITY],\n",
    "                ORG_COUNTRY: input_row[ORG_COUNTRY],\n",
    "                ORG_DEPT: input_row[ORG_DEPT],\n",
    "                ORG_DISTRICT: input_row[ORG_DISTRICT],\n",
    "                ORG_DUNS: input_row[ORG_DUNS],\n",
    "                ORG_FIPS: input_row[ORG_FIPS],\n",
    "                ORG_IPF_CODE: input_row[ORG_IPF_CODE],\n",
    "                ORG_NAME: input_row[ORG_NAME],\n",
    "                ORG_STATE: input_row[ORG_STATE],\n",
    "                ORG_ZIPCODE: input_row[ORG_ZIPCODE],\n",
    "                PHR: input_row[PHR],\n",
    "                PI_IDS: input_row[PI_IDS],\n",
    "                PI_NAMES: input_row[PI_NAMES],\n",
    "\n",
    "                NB_PI_IDS: len(pi_ids_list),\n",
    "                NB_PI_NAMES: len(pi_names_list),\n",
    "                CONTACT_PI_ID: contact_pi_id, \n",
    "                CONTACT_PI_NAME: contact_pi_name, \n",
    "                CONTACT_PI_ID_STATUS: pi_id_status, \n",
    "                CONTACT_PI_NAME_STATUS: pi_name_status,\n",
    "                CONTACT_PI_FIRST_NAME: contact_pi_first_name,\n",
    "                CONTACT_PI_GENDER: gender_dict.get(contact_pi_first_name),\n",
    "\n",
    "                PROGRAM_OFFICER_NAME: input_row[PROGRAM_OFFICER_NAME],\n",
    "                PROJECT_START: input_row[PROJECT_START],\n",
    "                PROJECT_END: input_row[PROJECT_END],\n",
    "                PROJECT_TERMS: input_row[PROJECT_TERMS],\n",
    "                PROJECT_TITLE: input_row[PROJECT_TITLE],\n",
    "                SERIAL_NUMBER: input_row[SERIAL_NUMBER],\n",
    "                STUDY_SECTION: input_row[STUDY_SECTION],\n",
    "                STUDY_SECTION_NAME: input_row[STUDY_SECTION_NAME],\n",
    "                SUBPROJECT_ID: input_row[SUBPROJECT_ID],\n",
    "                SUFFIX: input_row[SUFFIX],\n",
    "                SUPPORT_YEAR: input_row[SUPPORT_YEAR],\n",
    "                DIRECT_COST_AMT: input_row[DIRECT_COST_AMT],\n",
    "                INDIRECT_COST_AMT: input_row[INDIRECT_COST_AMT],\n",
    "                TOTAL_COST: input_row[TOTAL_COST],\n",
    "                TOTAL_COST_SUB_PROJECT: input_row[TOTAL_COST_SUB_PROJECT]\n",
    "            }\n",
    "            writer.writerow(dict_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing and building SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design is being built here: https://dbdiagram.io/d/61fd59c585022f4ee53e2950\n",
    "\n",
    "Database type: Microsoft SQL Server\n",
    "\n",
    "Purpose: usage by the CRC team members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(source_file, target_file, subset_headers):\n",
    "    \n",
    "    subset_headers_set = set(subset_headers)\n",
    "    \n",
    "    with open(source_file, 'r', encoding=ENCODING, newline=NEWLINE) as csv_file_descriptor:\n",
    "        reader = csv.DictReader(csv_file_descriptor, delimiter=DELIMITER)\n",
    "\n",
    "        with open(target_file, 'w', encoding=ENCODING, newline=NEWLINE) as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=subset_headers, delimiter=DELIMITER)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                subset_row = {}\n",
    "                for key, value in row.items():\n",
    "                    if key in subset_headers_set:\n",
    "                        subset_row[key] = value\n",
    "                writer.writerow(subset_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE PROJECTS\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'PROJECTS.tsv', [\n",
    "    APPLICATION_ID, \n",
    "    CORE_PROJECT_NUM, \n",
    "    ACTIVITY, \n",
    "    ADMINISTERING_IC, \n",
    "    APPLICATION_TYPE, \n",
    "    ARRA_FUNDED, \n",
    "    AWARD_NOTICE_DATE, \n",
    "    CFDA_CODE, \n",
    "    FOA_NUMBER, \n",
    "    FUNDING_ICS, \n",
    "    FUNDING_MECHANISM, \n",
    "    NIH_SPENDING_CATS, \n",
    "    ORG_IPF_CODE, \n",
    "    PHR, \n",
    "    PROGRAM_OFFICER_NAME, \n",
    "    PROJECT_START, \n",
    "    PROJECT_END, \n",
    "    PROJECT_TERMS, \n",
    "    PROJECT_TITLE, \n",
    "    SERIAL_NUMBER, \n",
    "    SUFFIX, \n",
    "    STUDY_SECTION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE SUBPROJECTS\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'SUBPROJECTS.tsv', [\n",
    "    SUBPROJECT_ID, \n",
    "    PROJECT_TITLE, \n",
    "    TOTAL_COST_SUB_PROJECT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE ACTIVITY\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'ACTIVITY.tsv', [\n",
    "    ACTIVITY, \n",
    "    'ACTIVITY_NAME']) #champ vide pour l'instant; à compléter avec documentation du NIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE APPLICATION_TYPE\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'APPLICATION_TYPE.tsv', [\n",
    "    APPLICATION_TYPE, \n",
    "    'APPLICATION_TYPE_NAME']) #champ vide pour l'instant; à compléter avec documentation du NIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE PRINCIPAL_INVESTIGATORS\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'PRINCIPAL_INVESTIGATORS.tsv', [\n",
    "    APPLICATION_ID, \n",
    "    PI_IDS, \n",
    "    PI_NAMES, \n",
    "    NB_PI_IDS, \n",
    "    NB_PI_NAMES, \n",
    "    CONTACT_PI_ID, \n",
    "    CONTACT_PI_NAME, \n",
    "    CONTACT_PI_FIRST_NAME, \n",
    "    CONTACT_PI_GENDER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE FUNDING_PER_FISCAL_YEAR\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'FUNDING_PER_FISCAL_YEAR.tsv', [\n",
    "    APPLICATION_ID, \n",
    "    FULL_PROJECT_NUM, \n",
    "    FY, \n",
    "    BUDGET_START, \n",
    "    BUDGET_END, \n",
    "    SUPPORT_YEAR, \n",
    "    DIRECT_COST_AMT, \n",
    "    INDIRECT_COST_AMT, \n",
    "    TOTAL_COST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE GRANTEE_ORGANIZATION\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'GRANTEE_ORGANIZATION.tsv', [\n",
    "    ORG_IPF_CODE, \n",
    "    ORG_DUNS, \n",
    "    ORG_NAME, \n",
    "    ORG_DEPT, \n",
    "    ORG_FIPS, \n",
    "    ORG_COUNTRY, \n",
    "    ORG_STATE, \n",
    "    ORG_DISTRICT, \n",
    "    ORG_CITY, \n",
    "    ORG_ZIPCODE, \n",
    "    ED_INST_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE INSTITUTE_OR_CENTER\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'INSTITUTE_OR_CENTER.tsv', [\n",
    "    ADMINISTERING_IC, \n",
    "    IC_NAME, \n",
    "    'IC_ACRONYM']) #champ vide pour l'instant; à compléter avec documentation du NIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE STUDY_SECTION\n",
    "filter_columns(ENHANCED_FILE, TABLE_DIR + 'STUDY_SECTION.tsv', [\n",
    "    STUDY_SECTION, \n",
    "    STUDY_SECTION_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TABLE_DIR + 'PROJECTS.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche testée mais rejetée\n",
    "\n",
    "On conservera plutôt les titres des champs du NIH même s'ils manquent parfois de clarté, pour faciliter la mise à jour et reprise du projet par d'autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def rename_headers(source_file, target_file, renamed_header_dict):\n",
    "    \n",
    "    with open(source_file, 'r', encoding='utf-8') as csv_file_descriptor:\n",
    "        reader = csv.DictReader(csv_file_descriptor, delimiter=',', quotechar='\"')\n",
    "\n",
    "        with open(target_file, 'w', encoding='utf-8') as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=renamed_header_dict.values(), dialect=csv.excel)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                new_row = {}\n",
    "                for old_header, value in row.items():\n",
    "                    new_header = renamed_header_dict[old_header] #syntaxe pour dire: prends la valeur sous la clé 'old_header'\n",
    "                    new_row[new_header] = value #ajouter nouvelle clé, valeur au dictionnaire new_row\n",
    "                writer.writerow(new_row)\n",
    "    \n",
    "\n",
    "def filter_and_rename(input_file, output_file, columns):    \n",
    "    #appeler les deux autres fonctions\n",
    "    temp_file_name = f'temp/{input_file}'\n",
    "    \n",
    "filter_and_rename('content/enhanced-output.csv', 'content/table-projects.csv', OrderedDict([\n",
    "    (APPLICATION_ID, APPLICATION_ID),\n",
    "    (CORE_PROJECT_NUM, CORE_PROJECT_NUM),\n",
    "    (ACTIVITY, 'ACTIVITY_CODE'),\n",
    "    (ADMINISTERING_IC, 'ADMINISTERING_IC_CODE'),\n",
    "    (APPLICATION_TYPE, 'APPLICATION_TYPE_CODE'),\n",
    "    (ARRA_FUNDED, ARRA_FUNDED)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "tester ce que le renamed_header_dict.values() fait, en créant un dictionnaire vide\n",
    "\n",
    "grosso modo ça crée une liste avec les valeurs du renamed_header_dict (qui va devenir notre orderedDict quand on va appeler la fonction plus loin)\n",
    "donc on va chercher le nouveau header qui est la valeur (on aurait fait renamed_header_dict.keys() si on avait voulu la clé, i.e. anciens headers)\n",
    "optionnellement rajouter avec un .lower()\n",
    "\n",
    "ensuite, continuer avec la fonction filter_and_rename\n",
    "puis compléter le orderedDict avec tous les headers pour la table Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NIH-RePORTER-preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
